<!doctype html>

<html>
<head>
    <link rel="stylesheet" type="text/css" href="css/moo.css">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>

<body>

    <h1>Samuel Tenka</h1>

    <div class="tab0">
        <button class="tablinks0" onclick="open_tab(event, 'Home', 0)">What's New</button>
        <button class="tablinks0" onclick="open_tab(event, 'Projects', 0)">Projects</button>
        <button class="tablinks0" onclick="open_tab(event, 'Exposition', 0)">Exposition</button>
        <button class="tablinks0" onclick="open_tab(event, 'Cows', 0)">Cows</button>
        <button class="tablinks0" onclick="open_tab(event, 'Schedule', 0)">Schedule</button>
        <button class="tablinks0" onclick="open_tab(event, 'Reading', 0)">Reading</button>
        <button class="tablinks0" onclick="open_tab(event, 'Statistics', 0)">Statistics</button>
    </div>
    
    <div id="Home" class="tabcontent0">
    </div>
    
    <div id="Projects" class="tabcontent0">
        <div class="tab1">
            <button class="tablinks1" onclick="open_tab(event, 'Music', 1)">Music</button>
            <button class="tablinks1" onclick="open_tab(event, 'Red Ball', 1)">Red Ball</button>
            <button class="tablinks1" onclick="open_tab(event, 'Eraser', 1)">Eraser</button>
            <button class="tablinks1" onclick="open_tab(event, 'Compiler', 1)">Compiler</button>
            <button class="tablinks1" onclick="open_tab(event, 'Chesstimator', 1)">Chesstimator</button>
        </div>

        <div id="Music Generator" class="tabcontent1">
        </div>
        <div id="Red Ball" class="tabcontent1">
        </div>
        <div id="Eraser" class="tabcontent1">
        </div>
        <div id="Compiler" class="tabcontent1">
        </div>
        <div id="Chesstimator" class="tabcontent1">
        </div>

    </div>
    
    <div id="Exposition" class="tabcontent0">
        <div class="tab1">
            <button class="tablinks1" onclick="open_tab(event, 'Boundary Topology', 1)">Boundary Topology</button>
            <button class="tablinks1" onclick="open_tab(event, 'Blass\' 7 Trees in 1', 1)">Blass' 7 Trees in 1</button>
            <button class="tablinks1" onclick="open_tab(event, 'Cotopology', 1)">Cotopology</button>
            <button class="tablinks1" onclick="open_tab(event, 'Deep Learning', 1)">Deep Learning</button>
            <button class="tablinks1" onclick="open_tab(event, 'Deep NLP', 1)">Deep NLP</button>
            <button class="tablinks1" onclick="open_tab(event, 'Differential Privacy', 1)">Differential Privacy</button>
            <button class="tablinks1" onclick="open_tab(event, 'Evolution Diffusion', 1)">Evolution Diffusion</button>
            <button class="tablinks1" onclick="open_tab(event, 'GANs', 1)">GANs</button>
            <button class="tablinks1" onclick="open_tab(event, 'General Relativity', 1)">General Relativity</button>
            <button class="tablinks1" onclick="open_tab(event, 'Grump', 1)">Grump</button>
            <button class="tablinks1" onclick="open_tab(event, 'Hat Problem', 1)">Hat Problem</button>
            <button class="tablinks1" onclick="open_tab(event, 'Linear Algebra for ML', 1)">Linear Algebra for ML</button>
            <button class="tablinks1" onclick="open_tab(event, 'Multiply without Multiplying', 1)">Multiply without Multiplying</button>
            <button class="tablinks1" onclick="open_tab(event, 'Nomograms', 1)">Nomograms</button>
            <button class="tablinks1" onclick="open_tab(event, 'P vs NP', 1)">P vs NP</button>
            <button class="tablinks1" onclick="open_tab(event, 'Quantum Mechanics', 1)">Quantum Mechanics</button>
            <button class="tablinks1" onclick="open_tab(event, 'Robust Regression', 1)">Robust Regression</button>
            <button class="tablinks1" onclick="open_tab(event, 'Sparse Coding', 1)">Sparse Coding</button>
            <button class="tablinks1" onclick="open_tab(event, 'Substitution Sequences', 1)">Substitution Sequences</button>
            <button class="tablinks1" onclick="open_tab(event, 'VC and PAC', 1)">VC and PAC</button>
            <button class="tablinks1" onclick="open_tab(event, 'Virtue Learning', 1)">Virtue Learning</button>
            <button class="tablinks1" onclick="open_tab(event, 'Wiener Filter', 1)">Wiener Filter</button>
            <button class="tablinks1" onclick="open_tab(event, 'Zero-Indexing', 1)">Zero-Indexing</button>
        </div>

        <div id="Boundary Topology" class="tabcontent1">
    <p>
$\newcommand{\CC}{\mathbb{C}}$
$\newcommand{\RR}{\mathbb{R}}$
$\newcommand{\ZZ}{\mathbb{Z}}$
$\newcommand{\Oo}{\mathcal{O}}$
$\newcommand{\Cc}{\mathcal{C}}$
$\newcommand{\Pow}{\mathcal{P}}$
$\newcommand{\bd}{\partial}$
$\newcommand{\minus}{\setminus}$
$\newcommand{\from}{\leftarrow}$
$\newcommand{\norm}[1]{\left\|#1\right\|}$
$\newcommand{\wrap}[1]{\left(#1\right)}$
$\newcommand{\wabs}[1]{\left|#1\right|}$
$\newcommand{\clor}[1]{\overline{#1}}$
$\newcommand{\intr}[1]{\underline{#1}}$
$\newcommand{\mtrx}[1]{\wrap{\begin{matrix} #1 \end{matrix}}}$
    Can we reformulate point-set topology in terms of
    topological boundaries rather than open or closed sets?
    We describe such a reformulation, invented with Dean Young,
    Noah Luntzlara, Eric Winsor, Leo Izen, and Brian Pinsky.
    </p>

    <p>
    We later learned of previous work by <a href="https://arxiv.org/abs/math/0605259">Lesniak</a>.
    Lesniak's Example 2 shows the forward and backward correspondences
    developed here not to be bijective. In particular,
    our axiom set is missing the logically independent
    $\bd \bd U \subseteq \bd U$ necessary to make unique
    the boundary operator associated with a topological space.
    We nevertheless present topology from a boundary point
    of view for whatever insight it may give.
    </p>
    
    <h2> Introduction </h2>
        <p>
        As scientists studying space, we might begin with 
        this observation: a region of space has an inside and
        an outside separated by a boundary. What can we say
        then, about boundaries? We note that space on whole
        has empty boundary: space is entirely inside with
        no outside. Moreover, a region and its complement
        have the same boundary.
        </p>

        <p>
        Yet, those observations fail to capture the essence
        of a boundary as a thin tissue separating a region
        and its complement. We express this essence by considering
        the boundary of an intersection. As we see in the Figure,
        the boundary of the depicted intersection  
        is a union of three terms, akin to the product rule
        for finite differences:
            $$d(ab) = da b + a db + da db$$
        </p>

        <div>
        <center>
            <img src="imgs/prod.png">
            <div> Figure 0: The Product Rule </div>
        </center>
        </div>

        <p>
        Let us collect our intuitions into a definition:
        </p>
        <p>
            <strong>Definition 0</strong>:
            A map $\bd: \Pow T \from \Pow T$ on a set $T$ is
            a <strong>boundary operator</strong> provided that, for all
            $U, V\subseteq T$:
                <ul style="list-style:none">
                    <li> <strong>boundless</strong> $\bd(T) = \{\}$
                    <li> <strong>symmetry</strong> $\bd(T\minus U) = \bd U$
                    <li> <strong>product</strong> $\bd(U V) \subseteq \bd U V \cup U \bd V \cup \bd U \bd V$
                </ul>
            We then call $T$ a <strong>boundless space</strong>.
            Here, concatenation denotes intersection: $UV = U\cap V$.
        </p>
            <strong>Definition 1</strong>:
            For $T$ a boundless space and $U \subseteq T$, we say $U$ is:
                <ul style="list-style:none">
                    <li> <strong>boundless</strong> when $\bd U = \{\}$ 
                    <li> <strong>open</strong> when $\bd U \subseteq T\minus U$
                    <li> <strong>closed</strong> when $\bd U \subseteq U$ 
                </ul>
            The boundless sets are precisely the open closed sets.
        </p>

        Our main result is a correspondence between boundless spaces and topological spaces:
        
        <p>
        <strong>Theorem 0</strong>:
        <em>
            The open sets of a boundless space form a topology, and the
            standard boundary map of a topological space is a boundary
            operator.
        </em>
        We prove this in the next section.
        </p>

        <p>
        Note that the axioms of symmetry, product, and boundlessness all involve
        finitary set operations; in particular, though they imply the closure
        of open sets under arbitrary union, they make no reference to arbitrary unions.
        We observe a pleasing correspondence between the three boundary axioms
        in Definition 0 and the symbols $T$, $\neg$, and $\wedge$ of
        propositional logic. Because the source and target of $\bd$ agree,
        we may chain the axioms into wordless but rigorous proofs. 
        </p>
    
    <h2> Proof of Correspondence </h2>
        We now prove the Theorem.
            Suppose $(T, \bd: \Pow T \from \Pow T)$ is a boundless space.
            Let $\Oo = \{U\subseteq T: \bd U = \{\}\}$ be its open sets.
            We check that $\Oo$ forms a topology on $T$.

            By boundlessness and symmetry, $\{\}$ and $T$ are open.
            Product rule implies that $\bd(U V) \subseteq \bd U \cup \bd V$,
            from which we deduce the closure of open sets under finitary intersection.
            Indeed, if $U, V$ are open, that is, $U \bd U = V \bd V = \{\}$, then
            \begin{align*}
                           (U V) \bd(U V)
                &\subseteq (U V) (\bd U \cup \bd V) \\
                &\subseteq (U \bd U) \cup (V \bd V) \\
                & =        \{\}
            \end{align*}
            To reason about unions, we transform the product rule using De Morgan's Law and
            symmetry: 
            \begin{align*}
                \bd(U\cup V) \subseteq \wrap{\bd U \minus V} \wrap{\cup \bd V \minus U} \cup \bd U \bd V
            \end{align*}
            In particular, for all $U, V\subseteq T$:
            \begin{align*}
                U \bd(U\cup V) \subseteq \bd U 
            \end{align*}
            So let $\Cc \subseteq \Oo$ be a collection of open sets. We know $U \bd U=\{\}$
            for each $U\in \Cc$. Let $V = \bigcup_{U\in\Cc} U$. Then 
            \begin{align*}
                           V \bd(V)
                &=         \bigcup_{U\in \Cc} U U \bd(U\cup V) \\ 
                &\subseteq \bigcup_{U\in \Cc} U \bd U \\
                &=         \{\}
            \end{align*}
            So $V$ is open. We conclude $\Oo$ forms a topology on $T$.

            Conversely, suppose $(T, \Oo \subseteq \Pow T)$ is a topological
            space. We define the familiar topological boundary as follows.
            For $V \subset T$, let
                closure $\clor{V} = \bigcap_{\substack{U\in\Oo\\T\minus U \supseteq V}} T\minus U$
                interior $\intr{V} = \bigcup_{\substack{U\in\Oo\\U \subseteq V}} U$
                boundary $\bd V = \clor{V} \minus \intr{V}$
            Then $(T, \bd)$ is a boundless space.

            Indeed, to see boundlessness, observe $\clor{T} = T = \intr{T}$, so $\bd T = \{\}$.
            Symmetry follows since closure and interior are conjugates under complement.
            The product rule we prove via two lemmas.
            We use that closure and interior are nondecreasing, and also that
            $\clor{V} \supseteq V \supseteq \intr{V}$.
            \begin{align*}
                           \bd(UV)
                &=         \clor{UV} \minus \intr{UV} \\
                &\subseteq (\clor{U})(\clor{V}) \minus (UV) \\
                &=         (\clor{U})(\clor{V})(T\minus U \cup T\minus V) \\
                &=         (\clor{U}\minus U) \cup (\clor{V}\minus V) \\
                &=         \bd U \cup \bd V
            \end{align*}
            And 
            \begin{align*}
                           U \bd(U\cup V)
                &=         U (\clor{U\cup V}\minus \intr{U\cup V}) \\
                &\subseteq U \minus (\intr{U}\cup\intr{V}) \\
                &\subseteq \clor{U} \minus \intr{U} \\
                &=         \bd U
            \end{align*}
            The latter result shows, by De Morgan's Law and symmetry, that: 
            \begin{align*}
                           \bd(UV)
                &\subseteq \bd(U) \cup U 
            \end{align*}
            So:
            \begin{align*}
                           \bd(UV)
                &\subseteq (\bd U \cup \bd V)(\bd U \cup U) \\
                &=         \bd U \cup (U \bd V)
            \end{align*}
            Switching the roles of $U$ and $V$:
            \begin{align*}
                           \bd(UV)
                &\subseteq (\bd U \cup (U\bd V))(\bd V \cup (V \bd U)) \\
                &=         U\bd V \cup \bd U V \cup \bd U \bd V
            \end{align*}
            Consequently, $\bd$ obeys product rule. So $T$ is a boundless space.

    <h2> Compactness </h2>

        </div>
        <div id="Blass' 7 Trees in 1" class="tabcontent1">
        </div>
        <div id="Cotopology" class="tabcontent1">
        </div>
        <div id="Deep Learning" class="tabcontent1">
        </div>
        <div id="Deep NLP" class="tabcontent1">
        </div>
        <div id="Differential Privacy" class="tabcontent1">
        </div>
        <div id="Evolution Diffusion" class="tabcontent1">
        </div>
        <div id="GANs" class="tabcontent1">
        </div>
        <div id="General Relativity" class="tabcontent1">
        </div>
        <div id="Grump" class="tabcontent1">
        </div>
        <div id="Hat Problem" class="tabcontent1">
        </div>
        <div id="Linear Algebra for ML" class="tabcontent1">
$\newcommand{\from}{\leftarrow}$
$\newcommand{\CC}{\mathbb{C}}$
$\newcommand{\RR}{\mathbb{R}}$
$\newcommand{\ZZ}{\mathbb{Z}}$
$\newcommand{\norm}[1]{\left\|#1\right\|}$
$\newcommand{\wrap}[1]{\left(#1\right)}$
$\newcommand{\wabs}[1]{\left|#1\right|}$
$\newcommand{\wanteq}{\overset{!}{=}}$
$\newcommand{\mtrx}[1]{\wrap{\begin{matrix}#1\end{matrix}}}$
$\DeclareMathOperator{\Hom}{Hom}$
$\DeclareMathOperator{\trace}{trace}$
$\DeclareMathOperator{\sign}{sign}$

    <h2> Linear Algebra as Language </h2>
        <h3> $A$'s Story </h3>
            <p>
            Who is the average Michigan student $A$?
            </p>

            <p>
            Well, $A$ is 5-and-a-half feet tall and wears a medium T-shirt.
            This T-shirt is an altogether unimpressive green, that sickly
            mixture of maize and navy that recollects drowned crops; in
            slightly darker green is printed "Michigan Democratic Squirrel
            Ball Volunteers Club", superposed against a logo of Rorschachian
            ambiguity.
            </p>

            <p>
            Among the pedestrians and bicyclists of Central Campus, $A$ travels
            on a unicycle. Transportation is crucial because $A$'s classes
            occur mainly in the Nichols Arboretum between Central and North. In
            those classes, $A$ always earns a C. Despite this, $A$ sometimes 
            earns an A.
            </p>

            <p>
            $A$ enjoys swimming for sport: 10-and-a-quarter laps at the pool,
            whereupon they find themselves stuck in the liquid's middle.
            $A$ swims at 1 yard per second but 2 seconds per yard, and at this
            meager pace, still wins 8-contestant races one out of eight times.   
            </p>
            <p>
                <strong> Exercise UNKNOWN </strong>:
                Resolve the mystery of $A$'s speed by constructing an example.
                That is, construct a set of speeds that average to $1$ but
                whose inverses average to $2$. 
            </p>

        <h3> Vectors </h3>
            <p>
            $A$'s story both inspires and warns. It inspires because it shows
            that summarization is possible: a few statistics can capture
            something of the flavor of campus life. Do <a href="https://en.wikipedia.org/wiki/University_of_Michigan"> Wikipedia's opening
            3 paragraphs on the University</a> even compete? But it also warns us
            of ways by which summarization may mislead: most obviously, through
            the neglect of potentially crucial variety, and, more subtly,
            through the opportunity for contradictions.

            <p>
            With this double moral in mind, let's plunge ahead. <strong>Linear
            algebra</strong> is the language of weighted averages. Weighted averages
            can be constructed from addition and scaling:
                $$A = \sum_{0\leq i \lt n} \text{weight}_i \text{student}_i$$
            A <strong>vector space</strong> is a set equipped with notions of addition and
            scaling. More formally, we define a <strong>vector space</strong> as a set $V$
            equipped with an addition function $+:V\from V\times V$ and a
            scaling function $\cdot:V\from \RR\times V$ that obey the familiar
            algebraic laws. (Namely: $+$ should be associative and
            commutative with identity and inverse, and $\cdot$ should
            distribute over $+$ and $\RR$'s addition, associate with $\RR$'s
            multiplication, and obey $1\cdot v=v$. The $\RR$ can be generalized
            to other number systems, but we won't bother.)
            </p>

            <p>
            This mathematical structure allows the definition not only of
            averages but also of notions such as analogies. Those notions are
            captured by <strong>(linear) relations</strong>, that is, equations between
            vectors involving only addition and scaling. For instance, imagine
            a vector space $W$ of concepts. Here's a relation that might then
            hold:
                $$\text{queen} - \text{woman} = \text{king} - \text{man}$$
            Linear relationships are simple enough to analyze, but powerful enough
            to model real data. Combined with nonlinearities, those relationships
            underlie famous ML techniques such as Neural Networks, Kernel SVMs, 
            Gaussian Processes, Principal Component Analysis, and Word Vectors.
            </p>

            <p>
            Later, we'll introduce more structure, 
            allowing discussion of <strong>similarity</strong> between records,
            and hence of generalization. All these ingredients come
            together into a language appropriate for machine learning.  
            </p>

            <p>
            <strong>Exercise UNKNOWN</strong>:
                <em>How does abstraction help or hinder our thinking?</em>
            </p>

            <p>
            The language of linear algebra abstracts datapoints
            as elements of a vector space.
            </p>
                        
        <h3> Dual Vectors </h3>
            <p>
            We now know to represent data as vectors, but this is only
            half the story. The other half is the analysis of that data.
            Specifically, though we love vectors, at the end of the day,
            we want plain numbers. We want meaningful maps $f:\RR\from V$.
            Such maps are called <strong>features</strong>.
            </p>

            <p>
            For instance, recall the setting of equation UNKNOWN.
            We might study the royalty feature of $W$: $queen$ has royalty
            level $1$, $woman$ has royalty level $0$, and so on.
            Crucially, the analogy between concepts stays true of their
            royalty levels:
                $$1 - 0 = 1 - 0$$
            It is in this sense that a map $f:\RR\from W$ has meaning:
            it preserves all linear relations, or equivalently, all weighted 
            averages. When a function $f:\RR\from W$ preserves all linear
            relations, we call it a <strong>(linear) feature</strong>.
            (Linear features are also known as <strong>functionals</strong>,
            <strong>covectors</strong>, <strong>dual vectors</strong>, and even
            <strong>bras</strong> after the article of clothing.)
            Let $W^*$ denote the set of linear features on $W$. Then
            $W^*$ is also a real vector space, called the <strong>dual</strong> of $W$.
            </p>

        <h3> Linear Maps </h3>
            <p>
            Replacing $\RR$ by $V$, we consider maps $f:V\from W$ that preserve
            linear relations. The set of such maps is written $\Hom(V\from W)$,
            and itself forms a real vector space. We call its elements <strong>(linear) maps</strong>.
            It turns out that if $f:V\from W$ is a linear map and $f$ has an
            inverse $g$, then $g:W\from V$ is linear. In this case, we call $f$ 
            an <strong>isomorphism</strong>, we say $V$ and $W$ are <strong>isomorphic</strong>,
            and we write $V \cong W$. The {\bf identity map} $I:V\from V$ is an isomorphism.
            </p>
    
            <p>
            <strong>Exercise UNKNOWN</strong>:
                <em>Is $W^* \cong \Hom(\RR\from W)$ always true?</em>
            </p>

    <h2> Example: A Linear Classifier </h2>

    <h2> Similarity </h2>

        </div>
        <div id="Multiply without Multiplying" class="tabcontent1">
<h2>Multiply without Multiplying</h2>

Here's an interesting puzzle: given a broken calculator with working buttons
for only $1$, $+$, $-$, and $1/$, as well as $\pi$ and $e$, can we compute
$\pi \times e$?

When I first heard this, I mistakenly concluded it was impossible. Indeed,
using the above operations, we can get only rational functions of $\pi$ and
$e$, the degrees of which can't grow from any of the allowed operations.
Specifically: $\deg(1/f) = -\deg(f)$ and $\deg(f \pm g) = \max\{\deg(f), \deg(g)\}$
so it seems we can never get $\deg(\pi e)=2$ from $\deg(1)=0$ and
$\deg(\pi)=\deg(e)=1$.

However, one of my recent math homeworks required us to prove it is possible!
It turns out the previous argument doesn't work, since $\deg(f \pm g)$ can
actually be smaller than either $\deg(f)$ or $\deg(g)$ if the expressions
cancel. This allows us to escape degrees close to $0$ by canceling:
    $$\frac{1}{x}-\frac{1}{x+1}=\frac{1}{x^2+x}$$
So we can make $x^2$ and hence $2xy = (x+y)^2 - (x^2+y^2)$! From there, we can
divide by two as in $xy = 1/(1/2xy+1/2xy)$, yielding the marvelous formula:
$$xy = \frac{1}{\frac{1}{\left(\frac{1}{\frac{1}{(x+y)}-\frac{1}{(x+y)+1}}-(x+y)\right)-\left(\left(\frac{1}{\frac{1}{x}-\frac{1}{x+1}}-x\right)+\left(\frac{1}{\frac{1}{y}-\frac{1}{y+1}}-y\right)\right)}+\frac{1}{\left(\frac{1}{\frac{1}{(x+y)}-\frac{1}{(x+y)+1}}-(x+y)\right)-\left(\left(\frac{1}{\frac{1}{x}-\frac{1}{x+1}}-x\right)+\left(\frac{1}{\frac{1}{y}-\frac{1}{y+1}}-y\right)\right)}}$$
        </div>
        <div id="Nomograms" class="tabcontent1">
        </div>
        <div id="P vs NP" class="tabcontent1">
        </div>
        <div id="Quantum Mechanics" class="tabcontent1">
        </div>
        <div id="Robust Regression" class="tabcontent1">
$\newcommand{\RR}{\mathbb{R}}$
$\newcommand{\Ll}{\mathcal{L}}$
$\newcommand{\from}{\leftarrow}$
$\newcommand{\wrap}[1]{\left(#1\right)}$
$\newcommand{\wabs}[1]{\left|#1\right|}$
$\DeclareMathOperator{\deter}{det}$
$\DeclareMathOperator{\sign}{sign}$
$\DeclareMathOperator{\trace}{trace}$

<p>
  We propose a variant of Least Squares regression.
</p>
  
<p>
  <strong>Synopsis</strong>: To estimate a curve $f(\theta): \RR^m\from \RR^n$ from an $I$-indexed
               dataset $(Y: \RR^m \from I, X: \RR^n \from I)$, we find $\theta$ (and
               latent variables $\Sigma_i$ for $i\in I$) that maximizes:
                   $$p(Y) = \prod_{i\in I} \frac{\exp\wrap{-\trace{\wrap{\Sigma_i^{-1} \wrap{\Delta(i) \Delta(i)^T + \alpha S}}}/2}}{\wrap{\sqrt{(2\pi)^m \wabs{\deter \Sigma_i}}}^{1+\alpha}}$$
               where $\Delta(i) = Y(i)-f(\theta)(X(i))$. The hyperparameters here are $\alpha$, a positive
               number, and $S$, an $m\times m$ symmetric positive-definite matrix. As $\alpha$ grows, 
               we recover Least-Squares regression. $S$ should reflect the natural
               Mahalanobis distance on $\RR^m$ when such domain knowledge is available.
               This method may be more robust to outliers than Least Squares is. A 
               downside, however, is that its solutions are not always unique.
               Below, we present our path to proposing the above model, hence
               justifying why one should consider it to be a simple generalization of
               Least Squares.
</p>
  
<p>
  <strong>Least Squares</strong>: Regression works like this: given a parametrization $f: \wrap{\RR^m}^{\RR^n} \from \RR^d$
  of $d$-parameter functions to $\RR^m$ from $\RR^n$, a loss function $L: \RR \from \RR^m\times \RR^m$, and a training
  set $\wrap{Y:\RR^m \from I, X:\RR^n \from I}$ indexed by $I$, we wish to find
  $\theta\in \RR^d$ that makes $\Ll(\theta) = \sum_I L(f(\theta)(X(i)), Y(i))$ low.
</p>
  
<p>
  Often times, $L(y', y)$ has the form $\inf_{\Sigma} \log \wrap{1/p(y' | \Sigma, y)}$ for some
  family of densities parameterized by $\sigma$ and $y$. For instance, if $p$ is
  the family of Gaussians with mean $y$ and some fixed covariance matrix (here, 
  $\Sigma$ is $0$-dimensional), we obtain Least Squares Regression.
</p>
  
<p>
  <strong>Variation</strong>: But what if $p$ is the family of Gaussians with mean $y$ and covariance $\Sigma$?
  By allowing the model to distribute its uncertainty unevenly among datapoints, we
  expect to obtain a model more robust to outliers.
  We recall that $p(y' | \Sigma, y) = \sqrt{(2\pi)^{-m} \wabs{\deter \Sigma^{-1}}} \cdot \exp\wrap{-(y'-y)^T \Sigma^{-1} (y'-y)/2}$.
  Thus, $L(y', y)$ is an increasing function of $\inf_{\Sigma} \wrap{\log\wabs{\deter \Sigma} + (y'-y)^T \Sigma^{-1} (y'-y)}$. Let's find consider
  the case $m=1$ and find a $y'$-dependent minimum
  by differentiating with respect to $\Sigma$:
        $$0 = 1/\Sigma - (y'-y)^2/\Sigma^2$$
  So $\Sigma = \pm (y'-y)^2$, hence $\Sigma = (y'-y)^2$, hence
  $L(y', y) = 2\log (y'-y) + 1$. Once equipped with this modified loss function, we
  may forget about densities: we'd just minimize $\Ll(\theta)$.
</p>
  
<p>
  <strong>Correction</strong>: But this loss function is flawed! It's singular at $y'=y$, meaning that any regression
  curve that goes perfectly through a regression point automatically minimizes loss.
  Indeed, the model is so robust to outliers that it may focus entirely on one data point.
  A similar collapse occurs when fitting Gaussian Mixture Models. There, a natural
  fix is to use a conjugate prior and imagine some fake data in each cluster, say $\alpha$ datapoints of variance $S$. Effecting an analogous modification, we find
  $L(y', y) = \inf_{\Sigma} \wrap{(1+\alpha)\log\wabs{\deter \Sigma} + \trace\wrap{\Sigma^{-1} \wrap{(y'-y)(y'-y)^T + \alpha S}}}$. Then, when $m=1$, we have $\Sigma = \wrap{(y'-y)^2+S}/(1+\alpha)$ and hence
      $$L(y', y) = \log\wrap{(y'-y)^2 + \alpha S} + (1+\alpha) \frac{(y'-y)^2}{(y'-y)^2+ \alpha S} + \text{constant}$$
  Inspection shows that when $0 \lt S$, the above may cause $\Ll$ to have multiple local minima, but
  never singular minima. By proceeding in simplest steps within our proposed framework for
  regression, we have thus obtained a robust regressor; it is parameterized by one positive
  number $\alpha$ and one symmetric positive-definite $m \times m$ covariance matrix called $S$
  Its main flaw is the non convexity of its loss. 
</p>
  
<p>
  Example: suppose $m=1$, $n=0$, and our data set consists of two $y$-values: $-1$ and $+1$.
  If $f(\theta)(x) = f(\theta)(0) = \theta$, then how do the set of optimal $\theta$ vary with $S$?
  Well, up to an additive constant:
      $$\Ll(\theta) = \log\wrap{\wrap{(\theta-1)^2+\alpha S} \wrap{(\theta+1)^2+\alpha S}} +
                    (1+\alpha) \wrap{\frac{(\theta-1)^2}{(\theta-1)^2+\alpha S} + \frac{(\theta+1)^2}{(\theta+1)^2+\alpha S}}$$
  We would differentiate and graph the minima given more time. However, just by staring we
  can extract some insight: as $S$ approaches $0$, the set has size $2$ and its smaller and
  larger elements respectively approach $-1$ and $+1$. On the other hand, as $S$ gets large,
  we may approximate, up to an additive constant:
      $$\Ll(\theta) \approx \log(2\theta^2 \alpha S + \alpha^2 S^2) + (1+\alpha) 2\theta^2 / \alpha S$$
  This is minimized when
      $$0 = 4\theta/\alpha^2 S^2 + (1+\alpha) 4\theta/\alpha S$$
  that is, when $\theta=0$. This rough argument thus shows that, for sufficiently large $S$,
  the two local minima merge into one global minimum, by symmetry equal to $0$.
</p>
        </div>
        <div id="Sparse Coding" class="tabcontent1">
        </div>
        <div id="Substition Sequences" class="tabcontent1">
        </div>
        <div id="VC and PAC" class="tabcontent1">
        </div>
        <div id="Virtue Learning" class="tabcontent1">
        </div>
        <div id="Wiener Filter" class="tabcontent1">
        </div>
        <div id="Zero-Indexing" class="tabcontent1">
        </div>

    </div>

    <div id="Cows" class="tabcontent0">
    </div>

    <div id="Schedule" class="tabcontent0">

    <table style="width:100%">
        <tr>
            <th>Eastern Time</th> <th>Saturday</th> <th>Sunday</th> <th>Monday</th> <th>Tuesday</th> <th>Wednesday</th> <th>Thursday</th> <th>Friday</th>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 00:00 </td> <td bgcolor="#0000FF"></td> <td bgcolor="#0000FF"></td> <td bgcolor="#0000FF"></td> <td bgcolor="#0000FF"></td> <td bgcolor="#0000FF"></td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 00:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 01:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 01:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 02:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 02:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 03:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 03:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 04:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 04:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 05:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 05:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 06:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 06:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 07:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 07:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 08:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 08:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 09:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 09:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 10:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 10:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 11:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 11:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 12:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 12:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 13:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 13:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 14:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 14:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 15:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 15:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 16:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 16:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 17:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 17:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 18:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 18:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 19:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 19:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 20:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 20:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 21:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 21:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 22:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 22:30 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 23:00 </td>
        </tr>
        <tr>
            <td bgcolor="#0000FF"> 23:30 </td>
        </tr>
    </table>

    </div>

    <div id="Reading" class="tabcontent0">
    </div>

    <div id="Statistics" class="tabcontent0">
    </div>

    <script src="js/moo.js"></script>

</body>
</html>
