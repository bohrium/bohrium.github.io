# author: samtenka
# change: 2025-01-19
# create: 2025-01-19
# descrp:
#
#
#
#
#

! Precis of EECS 437
# !!!! Samuel Tenka, 2025

$\newcommand{\sfA}{{\mathsf{A}}}$
$\newcommand{\sfH}{{\mathsf{H}}}$
$\newcommand{\sfr}{{\mathsf{r}}}$
$\newcommand{\sfx}{{\mathsf{x}}}$
$\newcommand{\sfy}{{\mathsf{y}}}$
$\newcommand{\sfz}{{\mathsf{z}}}$
#
$\newcommand{\Ee}{{\mathbb{E}}}$
$\newcommand{\Rr}{{\mathbb{R}}}$
#
$\newcommand{\aA}{{\mathcal{A}}}$
$\newcommand{\fF}{{\mathcal{L}}}$
$\newcommand{\hH}{{\mathcal{H}}}$
$\newcommand{\nN}{{\mathcal{N}}}$
$\newcommand{\pP}{{\mathcal{P}}}$
$\newcommand{\xX}{{\mathcal{X}}}$
$\newcommand{\yY}{{\mathcal{Y}}}$
#
#
$\newcommand{\sto}{\rightsquigarrow}$

#---  _  ----------------------------------------------------------------------
!! Decision Theory
!!! Setup
Let's model decision-making in the face of uncertainty.  Nature's unknown
configuration \(\sfH\in\hH\) determines a reward landscape on the action-space
\(\aA\).  We take an action \(\sfA\in\aA\) based on noisy evidence
\(\sfy\in\yY\) of \(\sfH\).  Formally, the dependencies are{|((We write \(U\sto
V\) for the set \(U\to\pP^V\) of functions from \(U\) to the distributions on
\(V\), regarded as stochastic maps from \(U\) to \(V\).))|} \(p:\hH\sto\yY\)
and \(r:\hH\times\aA\sto\Rr\) and \(s:\yY\sto\aA\); we know \(p,r\) and must
design \(s\).

#   \(
#   \begin{tikzcd}
#   y \arrow[d, dashed] & H \arrow[l] \arrow[d] \\
#   A \arrow[r]         & r
#   \end{tikzcd}
#   \)

The data flow looks like this:
\[
\sfy\sim p(\sfH)
\qquad
\sfA\sim s(\sfy)
\qquad
\sfr\sim r(\sfA,\sfH)
\]
We want the reward \(\sfr\) to be high in expectation.  At this point, we can
take expectations with respect to \(\sfy,\sfA\) but not \(\sfH\), so we get
a reward function \(R_s:\hH\to\Rr\).

In other words, two methods for collapsing (aggregating) a bunch of values, one
for each \(H\), are: take the MIN (worst-case) or take the AVERAGE with respect
to some distribution.  That is, we evaluate the quality of a strategy \(s\) as
\(\min_\sfH{}R_s(\sfH)\) or as \(\Ee_{\sfH\sim\pi}R_s(\sfH)\).
#
These are two philosophically different ways of handling
uncertainty.

!!! Bayes
In the Bayes setup, we assume known a distribution \(\pi\in\pP^\hH\) that we
use as a prior on \(\hH\).  We want to \(\Ee_{\sfH\sim\pi}R_s(\sfH)\).

!!! Minimax
In the Minimax setup, we maximize \(\min_\sfH{}R_s(\sfH)\).

!!! Relationship via Duality

!!! Pictures for Binary Case

#---  _  ----------------------------------------------------------------------
!! Estimation and Information Geometry

!!! KL Geometry
Estimation fits into our starting setup with \(\hH\) and \(\aA\) both equal to
some continuous parameter space \(\xX\), and with \(r\) some notion of
closeness.  It thus behooves us to analyze this space \(\xX\) of distributions
on \(\yY\) geometrically.  The key notion of different-ness between two
distributions is the *KL Divergence*
\[
    D(p:q) = \Ee_{y\sim p}[\log(p(y)/q(y))]
\]
that measures our average excess surprise when we expect samples from \(q\) but
instead get samples from \(p\).  This divergence is *not symmetric*!  Also,
when the right-hand-side is not defined, we define the value to be infinite.

!!! Classical Quadratic Theory of Estimation

!!! Cramer-Rao Bound and Bias-Variance Tradeoff

!!! Multiple Parameters
Here are some interesting geometric effects when \(\xX\) has high dimension
(such as \(3\)).

!!!! Shrinkage
Take the standard Gaussian likelihood model: \(\sfy=x+\sfz\) where
\(\sfz\sim\nN(0,I)\).  The data of an estimator is the data of a vector field
\(v(y)\) inducing estimate \(\hat\sfx=y-v(y)\).  Then a leading approximation
for the square loss at \(x\) is
\[
    1 - 2 \text{div} v + \|v\|^2
\]
This approximation is valid when \(\text{div}v\) and \(v\cdot{}v\) both change
very slowly (compared to the natural length scale of \(sfz\)'s spread).  We
improve on the obvious estimator when
\[
\text{div}v\geq\|v\|^2/2
\]
The key is that the LHS has a chance of scaling with the dimension
to overwhelm the RHS.  Even for constant \(\|v\|\) (imagine a field of
unit-vectors), there's enough angular elbow room so that we can arrange for
divergences everywhere to be positive simultaneously.
#
((It's not enough to just argue by scaling that we can
make the RHS arbitrarily small by vertically scaling v, since we want an
improvement at every \(x\) %simultaneously%.))

For example, suppose \(\vec v(y) = u(\|y\|) (\vec y/\|y\|)\) is radial.  Then
(choose a basis at \(y\) that includes \(y/\|y\|\)) the divergence has a radial
term \(u\pr\) and \(m-1\) angular terms \(u/\|y\|\).  This angular contribution
represents that the surface area of radius-\(r\) balls decreases as \(r\)
decreases, by \(\text{SA}\pr/\text{SA}=(m-1)/r\).
#
Anyway, combining with \(\|v\|^2=u^2\) let's write a condition for improvement:
\[
    u\pr + ((m-1)/r) u \geq u^2/2
\]
Under the ansatz \(u(r)=\alpha r^p\), this condition reads \((p+m-1) r^{p-1}
\geq (\alpha/2) r^{2p}\), or \(r^{p+1}\leq 2/(\alpha(p+m-1))\).  Pay special
attention to \(p=1,-1\), which correspond respectively to Gaussian and certain
%unnormalized%-powerlaw priors.
#
For example, setting \(u(r)=\alpha r\) (the gaussian prior case) satisfies this
bound when \(m\geq \alpha r^2/2\), that is for \(r\ll \sqrt{m/\alpha}\).  We'd
like to modify this to have the same behavior.

An elegant way to get both behaviors is \(u(r) = \alpha r/(R^2+r^2)\), which for
\(R\gg 1\) much larger that \(z\)'s noise scale and \(\alpha\ll 1\) verifies
the small derivative assumptions on \(\text{div}v\) and \(v^2\).

This is a (regularized) *James Stein estimator*.  It corresponds to a certain
%unnormalized%-Cauchy prior.

#!!!! Nuisance Parameters
#!!!! Curvature

#---  _  ----------------------------------------------------------------------
!! Asymptotics
!!! 
!!! 

#---  _  ----------------------------------------------------------------------
!! Inference Methods: Stochastic Approximation
!!! 
!!! 

#---  _  ----------------------------------------------------------------------
!! Special Topics
!!! Universal Featurization
!!! Diffusion Models


!! Appendix: Math Helpers
Here are some math concepts we'll use.  Plus, as a %lagniappe%, some we won't.

!!! Basic Linear Algebra
!!!!
!!!!
!!!! Convexity and Duality

!!! Basic Calculus
!!!! Jacobian, Gradient
!!!! Hessian, Laplacian
!!!! Aside on Gradient Descent
!!!! Connections and Curvature

!!!! Topology
#Abstraction is the removal of irrelevant detail; generalization, the
#inclusion of

Let's think about subsets of the Cartesian plane.  A collection of equations
like \(x=y\) or \(x^2+y^2=1\) or \(\max(1,x^2+y^2)=1\) determines a plane
subset of simultaneous solutions.  More precisely, we require each equation to
read \(f(x,y)=0\) where \(f\) is some *continuous* function to the reals.  Call
a subset that arises in this way @Clean@.  The @Clean@ subsets are closed under
arbitrary intersection.  And by multiplication they are also closed under
finitary union.



The notions we'll use are *closure*.

Intuitively, the closed sets are those cut out by sets of equations such as

!!!!! Compactness
Suppose an infinite family of closed sets has empty intersection.  One way this
can happen is if a finite sub-family has empty intersection.  In this case, we
say the infinitary intersection is empty for @silly reasons@.
#
A set is *compact* when every empty infinitary intersection of closed sets is
empty for silly reasons.
#
The significance of compactness comes from two consequences: (a) that compact
sets push forward under continuous maps to compact sets; (b) that


!!! Basic Statistics
!!!! Axioms of Expectation
Consider a set \(X\) and an assignment \(E:\fF\to\Rr\) of real
numbers to each function in some set \(\fF\subseteq X\to\Rr\).
We insist that \(\fF\) contains all constant functions and is closed
under linear combination, and that:
#
\(E((x\mapsto c))=c\) and
#
\(E(f+\lambda g)=E(f)+\lambda{}E(g)\).
#
We also insist on monotonicity: that when \(f\leq{}g\) pointwise then
\(E(f)\leq{}E(g)\).
Such a system is @Self-Consistent Assignment of Expectation Values@.

From such an Assignment we can read off probabilities (the expectation value of
indicator functions) of sets that are well-behaved enough for their indicator
functions to be contained in \(F\).

!!!! Dis-integration ; Conditional and Marginal
!!!! Independence and Averaging
!!!! Moments and Cumulants
!!!! Aside on Concentration


!!! Basic Physics
!!!! Extremal Principles
Why do physics classes focus so much on %how things move%?  There's probably a
good reason, but I still find it fun to imagine an alternative: the physics of
%which configurations don't move%.  The key fact that allows us to make
quantitative predictions is this: *the universe is cold*!  So in many
situations we are permitted to assume that energy is minimized; this puts
strong constraints on what the stable configuration(s) are, as well as a
criterion for instability.

More generally, if \(Q\) is an extensive conserved quantity, then we may expect

# (By the way, the formal substitution \(-\beta\leftrightarrow{}i/\hbar\) gives
# a quantitative correspondence between thermodynamics and quantum mechanics, a
# profound bridge conjectured by P.Dirac and developed by R.Feynman.)

!!!! Harmonic Oscillators, Quadratics and Gaussians
!!!! Ising-Stanley Models and Critical Phenomena
# !!!!!
# !!!!! Polymers and Self-Avoiding Walks
